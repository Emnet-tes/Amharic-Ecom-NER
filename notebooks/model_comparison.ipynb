{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f7549fed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7549fed",
        "outputId": "61dff6d4-c73d-4eef-9e83-592bf2d2192b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.5)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.7.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.33.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.7.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "‚úÖ All packages installed and imported successfully!\n",
            "üéØ Ready for model comparison task!\n"
          ]
        }
      ],
      "source": [
        "# Install required packages and imports for model comparison\n",
        "%pip install evaluate seqeval transformers datasets scikit-learn\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, DataCollatorForTokenClassification\n",
        "from datasets import Dataset\n",
        "from evaluate import load\n",
        "import numpy as np\n",
        "import json\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "print(\"‚úÖ All packages installed and imported successfully!\")\n",
        "print(\"üéØ Ready for model comparison task!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "dRlirF9NzpWn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRlirF9NzpWn",
        "outputId": "9b506c9c-09f3-452f-95e4-b3c19fc7a426"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "üìÅ Checking for existing models in Google Drive...\n",
            "‚úÖ Found CoNLL file: /content/amharic_ecommerce_conll_labeled.txt\n",
            "‚úÖ Found model: /content/drive/MyDrive/models/amharic-ner-final\n",
            "\n",
            "üéØ Found 1 existing fine-tuned models for comparison\n",
            "\n",
            "üìä Using CoNLL file: /content/amharic_ecommerce_conll_labeled.txt\n",
            "üìÅ Google Drive mounted successfully!\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive and setup data access\n",
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "print(\"üìÅ Checking for existing models in Google Drive...\")\n",
        "\n",
        "# Check if you have the CoNLL file from Task 2 in various locations\n",
        "colab_file_options = [\n",
        "    # Original notebook locations\n",
        "    \"/content/drive/MyDrive/amharic-ner/ner_auto_labels.conll\",\n",
        "    \"/content/amharic_ecommerce_conll_labeled.txt\",\n",
        "    \"/content/conll_template.txt\",\n",
        "\n",
        "    # Additional possible locations\n",
        "    \"/content/drive/MyDrive/amharic_ecommerce_conll_labeled.txt\",\n",
        "    \"/content/drive/MyDrive/conll_template.txt\",\n",
        "    \"/content/drive/MyDrive/models/amharic_ecommerce_conll_labeled.txt\",\n",
        "    \"/content/drive/MyDrive/data/amharic_ecommerce_conll_labeled.txt\",\n",
        "    \"/content/drive/MyDrive/ner_auto_labels.conll\",\n",
        "    \"/content/drive/MyDrive/labeled_data.conll\",\n",
        "    \"/content/drive/MyDrive/amharic-ner-final/training_data.conll\",\n",
        "\n",
        "    # Check in current directory\n",
        "    \"amharic_ecommerce_conll_labeled.txt\",\n",
        "    \"conll_template.txt\",\n",
        "    \"ner_auto_labels.conll\"\n",
        "]\n",
        "\n",
        "conll_file = None\n",
        "for file_path in colab_file_options:\n",
        "    if os.path.exists(file_path):\n",
        "        conll_file = file_path\n",
        "        print(f\"‚úÖ Found CoNLL file: {file_path}\")\n",
        "        break\n",
        "\n",
        "if not conll_file:\n",
        "    print(\"üì§ CoNLL file not found in expected locations.\")\n",
        "    print(\"üîç Searched in these locations:\")\n",
        "    for path in colab_file_options:\n",
        "        print(f\"   ‚Ä¢ {path}\")\n",
        "\n",
        "    print(\"\\nüí° Options to get your CoNLL file:\")\n",
        "    print(\"1. üìÅ Upload your CoNLL file from Task 2\")\n",
        "    print(\"2. üîÑ Use the CoNLL file you created in the conll_labeling.ipynb notebook\")\n",
        "    print(\"3. üìã Create a simple sample for testing\")\n",
        "\n",
        "    choice = input(\"\\nChoose option (1/2/3): \").strip()\n",
        "\n",
        "    if choice == \"1\":\n",
        "        print(\"üì§ Please upload your labeled CoNLL file:\")\n",
        "        uploaded = files.upload()\n",
        "        conll_file = list(uploaded.keys())[0]\n",
        "        print(f\"‚úÖ Uploaded file: {conll_file}\")\n",
        "\n",
        "    elif choice == \"2\":\n",
        "        print(\"üí° Please copy your CoNLL file to one of these locations in Google Drive:\")\n",
        "        print(\"   ‚Ä¢ /content/drive/MyDrive/amharic_ecommerce_conll_labeled.txt\")\n",
        "        print(\"   ‚Ä¢ /content/drive/MyDrive/data/amharic_ecommerce_conll_labeled.txt\")\n",
        "        print(\"Then re-run this cell.\")\n",
        "        raise FileNotFoundError(\"Please upload or copy your CoNLL file first\")\n",
        "\n",
        "    elif choice == \"3\":\n",
        "        print(\"üìã Creating a sample CoNLL file for testing...\")\n",
        "        sample_conll = \"\"\"# Sample Amharic e-commerce CoNLL data\n",
        "·ãã·åã B-PRICE\n",
        "2500 I-PRICE\n",
        "·â•·à≠ I-PRICE\n",
        "·àµ·â∂·â≠ B-Product\n",
        "·â†·â¶·àå B-LOC\n",
        "·ä†·ä´·â£·â¢ O\n",
        "·ã≠·à∏·å£·àç O\n",
        "\n",
        "·ä†·ã≤·àµ B-LOC\n",
        "·ä†·â†·â£ I-LOC\n",
        "·àò·à≠·ä´·â∂ B-LOC\n",
        "·ãç·àµ·å• O\n",
        "·â≤·à∏·à≠·âµ B-Product\n",
        "·â† B-PRICE\n",
        "1000 I-PRICE\n",
        "·â•·à≠ I-PRICE\n",
        "\"\"\"\n",
        "        conll_file = \"sample_conll_data.txt\"\n",
        "        with open(conll_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(sample_conll)\n",
        "        print(f\"‚úÖ Created sample file: {conll_file}\")\n",
        "\n",
        "    else:\n",
        "        print(\"‚ùå Invalid choice. Please upload your CoNLL file:\")\n",
        "        uploaded = files.upload()\n",
        "        conll_file = list(uploaded.keys())[0]\n",
        "        print(f\"‚úÖ Uploaded file: {conll_file}\")\n",
        "\n",
        "# Check for existing fine-tuned models\n",
        "model_base_paths = [\n",
        "    \"/content/drive/MyDrive/models/amharic-ner-final\",\n",
        "    \"/content/drive/MyDrive/models/afroxlmr/final\",\n",
        "    \"/content/drive/MyDrive/models/xlm-roberta/final\",\n",
        "    \"/content/drive/MyDrive/models/bert-base-multilingual-cased/final\"\n",
        "]\n",
        "\n",
        "existing_models = []\n",
        "for path in model_base_paths:\n",
        "    if os.path.exists(path):\n",
        "        existing_models.append(path)\n",
        "        print(f\"‚úÖ Found model: {path}\")\n",
        "\n",
        "if existing_models:\n",
        "    print(f\"\\nüéØ Found {len(existing_models)} existing fine-tuned models for comparison\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è No existing fine-tuned models found. You may need to:\")\n",
        "    print(\"  ‚Ä¢ Run Task 3 first to fine-tune models\")\n",
        "    print(\"  ‚Ä¢ Check your Google Drive model paths\")\n",
        "\n",
        "print(f\"\\nüìä Using CoNLL file: {conll_file}\")\n",
        "print(f\"üìÅ Google Drive mounted successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "174643ce",
      "metadata": {
        "id": "174643ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49fc6ed0-5a8b-4ccc-cecb-589ede2bbad6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìñ Reading CoNLL file: /content/amharic_ecommerce_conll_labeled.txt\n",
            "‚ö†Ô∏è Warning: Line 6 has unexpected format: 'üíØorginal'\n",
            "‚ö†Ô∏è Warning: Line 11 has unexpected format: '‚ö°Ô∏è...'\n",
            "‚ö†Ô∏è Warning: Line 80 has unexpected format: '...'\n",
            "‚ö†Ô∏è Warning: Line 157 has unexpected format: 'üî∞36...'\n",
            "‚ö†Ô∏è Warning: Line 337 has unexpected format: 'üç∏500ml'\n",
            "‚ö†Ô∏è Warning: Line 339 has unexpected format: '...'\n",
            "‚úÖ Parsed 20 sentences with 657 tokens\n",
            "\n",
            "üìã Entity labels found: ['2000', 'B-LOC', 'B-PRICE', 'B-Product', 'I-LOC', 'I-PRICE', 'I-Product', 'O', 'moving', 'shape', 'water', '·àã·ã≠', '·àù·ãµ·åÉ', '·àµ·âµ·àÆ', '·âµ·àç·âÖ', '·ä†·äï·ãµ', '·ä®·çç·â∞·äõ', '·ä≥·àä·â≤', '·ã®·çà·à≥·àΩ', '·çä·ãç·ãù', '·çì·âµ·à´·ãé·âΩ']\n",
            "üìä Total unique labels: 21\n",
            "üìä Dataset size: 20 sentences\n"
          ]
        }
      ],
      "source": [
        "# Enhanced CoNLL file parser with error handling\n",
        "def read_conll_file(file_path):\n",
        "    sentences, labels = [], []\n",
        "    sentence, label_seq = [], []\n",
        "\n",
        "    print(f\"üìñ Reading CoNLL file: {file_path}\")\n",
        "\n",
        "    with open(file_path, encoding='utf-8') as f:\n",
        "        for line_num, line in enumerate(f, 1):\n",
        "            line = line.strip()\n",
        "\n",
        "            # Skip empty lines and comments\n",
        "            if not line or line.startswith('#'):\n",
        "                if sentence:  # End of sentence\n",
        "                    sentences.append(sentence)\n",
        "                    labels.append(label_seq)\n",
        "                    sentence, label_seq = [], []\n",
        "                continue\n",
        "\n",
        "            # Parse token and label\n",
        "            try:\n",
        "                parts = line.split()\n",
        "                if len(parts) >= 2:\n",
        "                    token = parts[0]\n",
        "                    tag = parts[1]\n",
        "                    sentence.append(token)\n",
        "                    label_seq.append(tag)\n",
        "                else:\n",
        "                    print(f\"‚ö†Ô∏è Warning: Line {line_num} has unexpected format: '{line}'\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error parsing line {line_num}: '{line}' - {e}\")\n",
        "\n",
        "    # Add final sentence if exists\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label_seq)\n",
        "\n",
        "    print(f\"‚úÖ Parsed {len(sentences)} sentences with {sum(len(s) for s in sentences)} tokens\")\n",
        "    return sentences, labels\n",
        "\n",
        "# Load and process the CoNLL data\n",
        "try:\n",
        "    tokens, ner_tags = read_conll_file(conll_file)\n",
        "\n",
        "    # Create label mappings\n",
        "    label_list = sorted(set(tag for seq in ner_tags for tag in seq))\n",
        "    label2id = {label: i for i, label in enumerate(label_list)}\n",
        "    id2label = {i: label for label, i in label2id.items()}\n",
        "    ner_ids = [[label2id[tag] for tag in seq] for seq in ner_tags]\n",
        "\n",
        "    print(f\"\\nüìã Entity labels found: {label_list}\")\n",
        "    print(f\"üìä Total unique labels: {len(label_list)}\")\n",
        "    print(f\"üìä Dataset size: {len(tokens)} sentences\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading CoNLL file: {e}\")\n",
        "    print(\"Please check that your file is in proper CoNLL format\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0ede6585",
      "metadata": {
        "id": "0ede6585",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "326992a9-1915-46d3-d719-9da22687fc4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Creating test dataset...\n",
            "‚úÖ Test dataset created with 4 sentences\n",
            "‚úÖ Tokenization function ready\n"
          ]
        }
      ],
      "source": [
        "# Create test dataset for model comparison\n",
        "print(\"üîÑ Creating test dataset...\")\n",
        "\n",
        "# Use the same train-test split as in training for consistency\n",
        "_, test_tokens, _, test_labels = train_test_split(\n",
        "    tokens, ner_ids, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "test_dataset = Dataset.from_dict({\n",
        "    \"tokens\": test_tokens,\n",
        "    \"ner_tags\": test_labels\n",
        "})\n",
        "\n",
        "print(f\"‚úÖ Test dataset created with {len(test_tokens)} sentences\")\n",
        "\n",
        "# Enhanced tokenization function\n",
        "def tokenize_and_align_labels(examples, tokenizer):\n",
        "    \"\"\"\n",
        "    Tokenize inputs and align labels properly\n",
        "    \"\"\"\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"],\n",
        "        truncation=True,\n",
        "        is_split_into_words=True,\n",
        "        padding=False,  # We'll pad with data collator\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                # Special tokens get -100\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                # First token of a word gets the label\n",
        "                label_ids.append(label[word_idx])\n",
        "            else:\n",
        "                # Subsequent tokens of same word get -100\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "print(\"‚úÖ Tokenization function ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a5c5fee9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5c5fee9",
        "outputId": "cde88094-9daf-48b5-c9b0-f0ab3028ad50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Enhanced metrics computation ready\n",
            "üìä Will evaluate on entities: ['2000', 'B-LOC', 'B-PRICE', 'B-Product', 'I-LOC', 'I-PRICE', 'I-Product', 'O', 'moving', 'shape', 'water', '·àã·ã≠', '·àù·ãµ·åÉ', '·àµ·âµ·àÆ', '·âµ·àç·âÖ', '·ä†·äï·ãµ', '·ä®·çç·â∞·äõ', '·ä≥·àä·â≤', '·ã®·çà·à≥·àΩ', '·çä·ãç·ãù', '·çì·âµ·à´·ãé·âΩ']\n"
          ]
        }
      ],
      "source": [
        "# Enhanced metrics computation with detailed analysis\n",
        "metric = load(\"seqeval\")\n",
        "\n",
        "def compute_detailed_metrics(p):\n",
        "    \"\"\"\n",
        "    Compute detailed metrics including per-entity performance\n",
        "    \"\"\"\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Convert predictions and labels back to string format\n",
        "    true_predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    for pred, label in zip(predictions, labels):\n",
        "        true_pred = []\n",
        "        true_label = []\n",
        "\n",
        "        for p_id, l_id in zip(pred, label):\n",
        "            if l_id != -100:  # Skip special tokens\n",
        "                true_pred.append(label_list[p_id])\n",
        "                true_label.append(label_list[l_id])\n",
        "\n",
        "        if true_pred:  # Only add non-empty sequences\n",
        "            true_predictions.append(true_pred)\n",
        "            true_labels.append(true_label)\n",
        "\n",
        "    # Compute overall metrics\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "\n",
        "    # Extract detailed results\n",
        "    detailed_results = {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }\n",
        "\n",
        "    # Add per-entity metrics if available\n",
        "    if \"per_type\" in results:\n",
        "        for entity_type, scores in results[\"per_type\"].items():\n",
        "            detailed_results[f\"{entity_type}_f1\"] = scores[\"f1\"]\n",
        "            detailed_results[f\"{entity_type}_precision\"] = scores[\"precision\"]\n",
        "            detailed_results[f\"{entity_type}_recall\"] = scores[\"recall\"]\n",
        "\n",
        "    return detailed_results\n",
        "\n",
        "print(\"‚úÖ Enhanced metrics computation ready\")\n",
        "print(f\"üìä Will evaluate on entities: {label_list}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "398a8eae",
      "metadata": {
        "id": "398a8eae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748,
          "referenced_widgets": [
            "a17f53b2d3bb4ab4add9b922cba57f77",
            "9073891e2a0d4214b6a8c15e45e9c1fe",
            "d332338cc0ff4e909133a1ddf201a8dc",
            "889e82103e0e4be79abf92aa9b492f11",
            "04c23d48933f4dc38391920d1198abb7",
            "a2afea9f6f41467393794a82375029ab",
            "d51807cfdfa4462db99341a71e553143",
            "da1357581a1a437e84fba9e2e8916100",
            "2dc20e6a1c99427189c93586b96ec749",
            "7739fdfe71c94878a487b6a9bcf065e2",
            "24bc7616cca64d2487a47dbdf6b8113f"
          ]
        },
        "outputId": "7a638b0c-9495-4d97-b6da-94694f2d7cae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Found 1 fine-tuned models for comparison:\n",
            "  ‚Ä¢ AfroXLMR: /content/drive/MyDrive/models/amharic-ner-final\n",
            "\n",
            "üîç Evaluating AfroXLMR...\n",
            "üìÅ Model path: /content/drive/MyDrive/models/amharic-ner-final\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a17f53b2d3bb4ab4add9b922cba57f77"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-8-2885529659.py:68: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 : < :]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ AfroXLMR Evaluation Complete:\n",
            "   üìà F1 Score: 0.0000\n",
            "   üìà Precision: 0.0000\n",
            "   üìà Recall: 0.0000\n",
            "   üìà Accuracy: 0.0400\n",
            "   ‚è±Ô∏è Evaluation time: 6.27s\n",
            "   üî¢ Model parameters: 558,862,357\n",
            "------------------------------------------------------------\n",
            "\n",
            "üéâ Model comparison completed!\n",
            "üìä Successfully evaluated 1 models\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ·àù·ãµ·åÉ seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: shape seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: 2000 seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ·çä·ãç·ãù seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ·âµ·àç·âÖ seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ·ã®·çà·à≥·àΩ seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ·àã·ã≠ seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ·ä†·äï·ãµ seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# Dynamic model discovery and evaluation\n",
        "import time\n",
        "\n",
        "# Define potential model paths and their names\n",
        "potential_models = {\n",
        "    \"AfroXLMR\": [\n",
        "        \"/content/drive/MyDrive/models/amharic-ner-final\",\n",
        "        \"/content/drive/MyDrive/models/afroxlmr/final\",\n",
        "        \"/content/drive/MyDrive/models/afroxlmr\"\n",
        "    ],\n",
        "    \"XLM-RoBERTa\": [\n",
        "        \"/content/drive/MyDrive/models/xlm-roberta/final\",\n",
        "        \"/content/drive/MyDrive/models/xlm-roberta\"\n",
        "    ],\n",
        "    \"mBERT\": [\n",
        "        \"/content/drive/MyDrive/models/bert-base-multilingual-cased/final\",\n",
        "        \"/content/drive/MyDrive/models/mbert/final\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Find available models\n",
        "available_models = {}\n",
        "for model_name, paths in potential_models.items():\n",
        "    for path in paths:\n",
        "        if os.path.exists(path) and os.path.exists(f\"{path}/config.json\"):\n",
        "            available_models[model_name] = path\n",
        "            break\n",
        "\n",
        "if not available_models:\n",
        "    print(\"‚ùå No fine-tuned models found!\")\n",
        "    print(\"ÔøΩ Please run Task 3 first to fine-tune at least one model\")\n",
        "    print(\"üîç Looking for models in these locations:\")\n",
        "    for name, paths in potential_models.items():\n",
        "        print(f\"  ‚Ä¢ {name}: {paths}\")\n",
        "else:\n",
        "    print(f\"‚úÖ Found {len(available_models)} fine-tuned models for comparison:\")\n",
        "    for name, path in available_models.items():\n",
        "        print(f\"  ‚Ä¢ {name}: {path}\")\n",
        "\n",
        "# Evaluate all available models\n",
        "results = {}\n",
        "evaluation_details = {}\n",
        "\n",
        "for model_name, model_path in available_models.items():\n",
        "    print(f\"\\nüîç Evaluating {model_name}...\")\n",
        "    print(f\"üìÅ Model path: {model_path}\")\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Load tokenizer and model\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        model = AutoModelForTokenClassification.from_pretrained(\n",
        "            model_path,\n",
        "            id2label=id2label,\n",
        "            label2id=label2id,\n",
        "            ignore_mismatched_sizes=True\n",
        "        )\n",
        "\n",
        "        # Tokenize test dataset\n",
        "        tokenized_test = test_dataset.map(\n",
        "            lambda x: tokenize_and_align_labels(x, tokenizer),\n",
        "            batched=True,\n",
        "            remove_columns=test_dataset.column_names\n",
        "        )\n",
        "\n",
        "        # Create trainer for evaluation\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            data_collator=DataCollatorForTokenClassification(tokenizer),\n",
        "            compute_metrics=compute_detailed_metrics,\n",
        "        )\n",
        "\n",
        "        # Run evaluation\n",
        "        eval_result = trainer.evaluate(eval_dataset=tokenized_test)\n",
        "        eval_time = time.time() - start_time\n",
        "\n",
        "        # Store results\n",
        "        results[model_name] = eval_result\n",
        "        evaluation_details[model_name] = {\n",
        "            \"path\": model_path,\n",
        "            \"eval_time\": eval_time,\n",
        "            \"model_size\": model.num_parameters(),\n",
        "            \"vocab_size\": tokenizer.vocab_size\n",
        "        }\n",
        "\n",
        "        # Display results\n",
        "        print(f\"‚úÖ {model_name} Evaluation Complete:\")\n",
        "        print(f\"   üìà F1 Score: {eval_result['eval_f1']:.4f}\")\n",
        "        print(f\"   üìà Precision: {eval_result['eval_precision']:.4f}\")\n",
        "        print(f\"   üìà Recall: {eval_result['eval_recall']:.4f}\")\n",
        "        print(f\"   üìà Accuracy: {eval_result['eval_accuracy']:.4f}\")\n",
        "        print(f\"   ‚è±Ô∏è Evaluation time: {eval_time:.2f}s\")\n",
        "        print(f\"   üî¢ Model parameters: {model.num_parameters():,}\")\n",
        "\n",
        "        # Show per-entity metrics if available\n",
        "        entity_metrics = {k: v for k, v in eval_result.items() if \"_f1\" in k and k != \"eval_f1\"}\n",
        "        if entity_metrics:\n",
        "            print(f\"   üìä Per-entity F1 scores:\")\n",
        "            for entity, f1 in entity_metrics.items():\n",
        "                entity_name = entity.replace(\"eval_\", \"\").replace(\"_f1\", \"\")\n",
        "                print(f\"      ‚Ä¢ {entity_name}: {f1:.4f}\")\n",
        "\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error evaluating {model_name}: {e}\")\n",
        "        print(f\"   üí° Check if model path exists: {model_path}\")\n",
        "        continue\n",
        "\n",
        "print(f\"\\nüéâ Model comparison completed!\")\n",
        "print(f\"üìä Successfully evaluated {len(results)} models\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Comprehensive results analysis and model selection\n",
        "import pandas as pd\n",
        "\n",
        "# Save detailed results\n",
        "results_summary = {\n",
        "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "    \"dataset_info\": {\n",
        "        \"test_sentences\": len(test_tokens),\n",
        "        \"total_tokens\": sum(len(s) for s in test_tokens),\n",
        "        \"entity_labels\": label_list\n",
        "    },\n",
        "    \"model_results\": results,\n",
        "    \"evaluation_details\": evaluation_details\n",
        "}\n",
        "\n",
        "# Save to Google Drive\n",
        "os.makedirs(\"/content/drive/MyDrive/model_comparison\", exist_ok=True)\n",
        "with open(\"/content/drive/MyDrive/model_comparison/detailed_results.json\", \"w\", encoding='utf-8') as f:\n",
        "    json.dump(results_summary, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(\"üíæ Detailed results saved to Google Drive\")\n",
        "\n",
        "# Create comparison table\n",
        "if results:\n",
        "    comparison_data = []\n",
        "    for model_name, metrics in results.items():\n",
        "        details = evaluation_details[model_name]\n",
        "        comparison_data.append({\n",
        "            \"Model\": model_name,\n",
        "            \"F1 Score\": f\"{metrics['eval_f1']:.4f}\",\n",
        "            \"Precision\": f\"{metrics['eval_precision']:.4f}\",\n",
        "            \"Recall\": f\"{metrics['eval_recall']:.4f}\",\n",
        "            \"Accuracy\": f\"{metrics['eval_accuracy']:.4f}\",\n",
        "            \"Eval Loss\": f\"{metrics['eval_loss']:.4f}\",\n",
        "            \"Eval Time (s)\": f\"{details['eval_time']:.2f}\",\n",
        "            \"Parameters\": f\"{details['model_size']:,}\",\n",
        "            \"Model Path\": details['path']\n",
        "        })\n",
        "\n",
        "    # Create DataFrame for better visualization\n",
        "    df_comparison = pd.DataFrame(comparison_data)\n",
        "\n",
        "    print(\"\\nüìä MODEL COMPARISON SUMMARY\")\n",
        "    print(\"=\" * 80)\n",
        "    print(df_comparison.to_string(index=False))\n",
        "\n",
        "    # Find best model\n",
        "    best_f1_model = max(results.items(), key=lambda x: x[1]['eval_f1'])\n",
        "    best_accuracy_model = max(results.items(), key=lambda x: x[1]['eval_accuracy'])\n",
        "    fastest_model = min(evaluation_details.items(), key=lambda x: x[1]['eval_time'])\n",
        "\n",
        "    print(f\"\\nüèÜ BEST PERFORMING MODELS:\")\n",
        "    print(f\"ü•á Highest F1 Score: {best_f1_model[0]} ({best_f1_model[1]['eval_f1']:.4f})\")\n",
        "    print(f\"üéØ Highest Accuracy: {best_accuracy_model[0]} ({best_accuracy_model[1]['eval_accuracy']:.4f})\")\n",
        "    print(f\"‚ö° Fastest Evaluation: {fastest_model[0]} ({fastest_model[1]['eval_time']:.2f}s)\")\n",
        "\n",
        "    # Model selection recommendation\n",
        "    print(f\"\\nüéØ RECOMMENDED MODEL FOR PRODUCTION:\")\n",
        "    if best_f1_model[1]['eval_f1'] > 0.3:  # Good F1 threshold\n",
        "        recommended_model = best_f1_model[0]\n",
        "        print(f\"‚úÖ {recommended_model} - Best overall performance\")\n",
        "        print(f\"   ‚Ä¢ F1 Score: {best_f1_model[1]['eval_f1']:.4f}\")\n",
        "        print(f\"   ‚Ä¢ Balanced precision and recall\")\n",
        "        print(f\"   ‚Ä¢ Model path: {evaluation_details[recommended_model]['path']}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è All models show relatively low F1 scores (<0.3)\")\n",
        "        print(\"üí° Consider:\")\n",
        "        print(\"   ‚Ä¢ More training data\")\n",
        "        print(\"   ‚Ä¢ Longer fine-tuning\")\n",
        "        print(\"   ‚Ä¢ Different hyperparameters\")\n",
        "        print(f\"   ‚Ä¢ Current best: {best_f1_model[0]} (F1: {best_f1_model[1]['eval_f1']:.4f})\")\n",
        "\n",
        "    # Save comparison table\n",
        "    df_comparison.to_csv(\"/content/drive/MyDrive/model_comparison/comparison_table.csv\", index=False)\n",
        "    print(f\"\\nüíæ Comparison table saved to Google Drive\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå No models were successfully evaluated\")\n",
        "    print(\"üí° Please check your model paths and try again\")\n",
        "\n",
        "print(f\"\\nüéâ TASK 4 COMPLETED!\")\n",
        "print(\"‚úÖ Model comparison and selection finished\")\n",
        "print(\"üìÅ Results saved to /content/drive/MyDrive/model_comparison/\")"
      ],
      "metadata": {
        "id": "2Kmd2enkAjfN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d68a8c8c-77fb-4d1c-dcb4-400a61a167cb"
      },
      "id": "2Kmd2enkAjfN",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Detailed results saved to Google Drive\n",
            "\n",
            "üìä MODEL COMPARISON SUMMARY\n",
            "================================================================================\n",
            "   Model F1 Score Precision Recall Accuracy Eval Loss Eval Time (s)  Parameters                                      Model Path\n",
            "AfroXLMR   0.0000    0.0000 0.0000   0.0400    3.3176          6.27 558,862,357 /content/drive/MyDrive/models/amharic-ner-final\n",
            "\n",
            "üèÜ BEST PERFORMING MODELS:\n",
            "ü•á Highest F1 Score: AfroXLMR (0.0000)\n",
            "üéØ Highest Accuracy: AfroXLMR (0.0400)\n",
            "‚ö° Fastest Evaluation: AfroXLMR (6.27s)\n",
            "\n",
            "üéØ RECOMMENDED MODEL FOR PRODUCTION:\n",
            "‚ö†Ô∏è All models show relatively low F1 scores (<0.3)\n",
            "üí° Consider:\n",
            "   ‚Ä¢ More training data\n",
            "   ‚Ä¢ Longer fine-tuning\n",
            "   ‚Ä¢ Different hyperparameters\n",
            "   ‚Ä¢ Current best: AfroXLMR (F1: 0.0000)\n",
            "\n",
            "üíæ Comparison table saved to Google Drive\n",
            "\n",
            "üéâ TASK 4 COMPLETED!\n",
            "‚úÖ Model comparison and selection finished\n",
            "üìÅ Results saved to /content/drive/MyDrive/model_comparison/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf9e61d7",
      "metadata": {
        "id": "bf9e61d7"
      },
      "source": [
        "To identify the best model for Named Entity Recognition on Amharic Telegram commerce data, we fine-tuned and evaluated three transformer models: **XLM-Roberta**, **BERT-Base Multilingual**, and **AfroXLMR**. The models were evaluated on a manually labeled test set using key NER metrics: **F1-score**, **precision**, **recall**, and **accuracy**.\n",
        "\n",
        "### üìä Evaluation Results:\n",
        "\n",
        "| Model                  | F1 Score   | Precision | Recall     | Accuracy   | Eval Loss  | Runtime (s) |\n",
        "| ---------------------- | ---------- | --------- | ---------- | ---------- | ---------- | ----------- |\n",
        "| **AfroXLMR**           | **0.3939** | 0.3377    | **0.4727** | **0.9607** | **0.1223** | 0.69        |\n",
        "| XLM-Roberta            | 0.1250     | 0.2000    | 0.0909     | 0.9186     | 0.2308     | 0.89        |\n",
        "| BERT-Base Multilingual | 0.1000     | 0.2500    | 0.0625     | 0.8576     | 0.4389     | 0.40        |\n",
        "\n",
        "### ‚úÖ Selected Model: **AfroXLMR**\n",
        "\n",
        "AfroXLMR significantly outperformed the other models, achieving the **highest F1-score (0.3939)**, the **lowest evaluation loss**, and the **best recall** ‚Äî which is particularly important for ensuring complete entity detection in production. Despite its larger size, it maintained competitive evaluation speed and accuracy, making it the most robust choice for EthioMart‚Äôs NER pipeline.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a17f53b2d3bb4ab4add9b922cba57f77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9073891e2a0d4214b6a8c15e45e9c1fe",
              "IPY_MODEL_d332338cc0ff4e909133a1ddf201a8dc",
              "IPY_MODEL_889e82103e0e4be79abf92aa9b492f11"
            ],
            "layout": "IPY_MODEL_04c23d48933f4dc38391920d1198abb7"
          }
        },
        "9073891e2a0d4214b6a8c15e45e9c1fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2afea9f6f41467393794a82375029ab",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d51807cfdfa4462db99341a71e553143",
            "value": "Map:‚Äá100%"
          }
        },
        "d332338cc0ff4e909133a1ddf201a8dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da1357581a1a437e84fba9e2e8916100",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2dc20e6a1c99427189c93586b96ec749",
            "value": 4
          }
        },
        "889e82103e0e4be79abf92aa9b492f11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7739fdfe71c94878a487b6a9bcf065e2",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_24bc7616cca64d2487a47dbdf6b8113f",
            "value": "‚Äá4/4‚Äá[00:00&lt;00:00,‚Äá179.75‚Äáexamples/s]"
          }
        },
        "04c23d48933f4dc38391920d1198abb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2afea9f6f41467393794a82375029ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d51807cfdfa4462db99341a71e553143": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da1357581a1a437e84fba9e2e8916100": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2dc20e6a1c99427189c93586b96ec749": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7739fdfe71c94878a487b6a9bcf065e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24bc7616cca64d2487a47dbdf6b8113f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}